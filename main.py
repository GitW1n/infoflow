import aiohttp
import asyncio
from bs4 import BeautifulSoup
import time
import requests
import datetime
from urllib.parse import urlparse
import pywhatkit
print(pywhatkit.__file__)

requirements = """–û–ø–∏—Å—ã–≤–∞–π—Ç–µ –≤—Å—ë –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –ö–æ–≥–¥–∞ –≤—ã –ø–æ–ª—É—á–∞–µ—Ç–µ —Ä–∞–∑–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ —Ç–æ –≤ —Ä–µ–∑—é–º–µ —Ä–∞–∑–¥–µ–ª—è–π—Ç–µ –∏—Ö, 
—Ç–∞–∫ —á—Ç–æ–±—ã –∫–∞–∂–¥–∞—è –Ω–∞—á–∏–Ω–∞–ª–∞—Å—å —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏ –∏ —Å–æ –∑–Ω–∞–∫–∞ '‚Äî'. 
–ö–æ–≥–¥–∞ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤—É–π—Ç–µ –≤ –∫–æ–Ω—Ü–µ –ø–µ—Ä–≤–æ–≥–æ –ø—É–Ω–∫—Ç–∞ —Å—Ç–∞–≤—å—Ç–µ –∑–Ω–∞–∫ üëã, –∫–æ–≥–¥–∞ –≥–æ–≤–æ—Ä–∏—Ç–µ –¥–∞—Ç—É –≤ –∫–æ–Ω—Ü–µ –≤—Ç–æ—Ä–æ–≥–æ –ø—É–Ω–∫—Ç–∞ —Å—Ç–∞–≤—å—Ç–µ –∑–Ω–∞–∫ ‚åõ, 
–∫–æ–≥–¥–∞ –≥–æ–≤–æ—Ä–∏—Ç–µ –æ –ø–æ–≥–æ–¥–µ –≤ –∫–æ–Ω—Ü–µ —Ç—Ä–µ—Ç—å–µ–≥–æ –ø—É–Ω–∫—Ç–∞ —Å—Ç–∞–≤—å—Ç–µ –∑–Ω–∞–∫ ‚òÇÔ∏è,
–∫–æ–≥–¥–∞ –≥–æ–≤–æ—Ä–∏—Ç–µ –æ —Ü–µ–Ω–∞—Ö –≤ –∫–æ–Ω—Ü–µ —á–µ—Ç–≤—ë—Ä—Ç–æ–≥–æ –ø—É–Ω–∫—Ç–∞ —Å—Ç–∞–≤—å—Ç–µ  –∑–Ω–∞–∫ üí∏.
–ù–æ–≤–æ—Å—Ç–∏ —Ä–∞–∑–¥–µ–ª—è–π—Ç–µ –Ω–∞: üóûÔ∏è –ü–æ–ª–∏—Ç–∏–∫–∞ –∏ –º–∏—Ä–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏, üìà –§–∏–Ω–∞–Ω—Å—ã&–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã&—ç–∫–æ–Ω–æ–º–∏–∫–∞, 
üíª–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ IT, üìä –ù–∞—É–∫–∞ –∏ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ, ‚öΩ –°–ø–æ—Ä—Ç

–ü–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –Ω–æ–≤–æ—Å—Ç–∏ –≤ —Å–∫–æ–±–∫–∞—Ö —É–∫–∞–∑—ã–≤–∞–π –∏—Å—Ç–æ—á–Ω–∏–∫(–µ—Å–ª–∏ –µ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è)
"""
news_url = "https://www.bbc.com/russian"

current_time = datetime.datetime.now()

def get_weather(city):
    url = f'https://yandex.com/weather/en/{city}'
    
    
    response = requests.get(url)
    
    if response.status_code != 200:
        print("–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö!")
        return
    
    
    soup = BeautifulSoup(response.text, 'html.parser')

    
    temp = soup.find('p', class_='AppFactTemperature_wrap__z_f_O')
    if temp:
        temperature = temp.find('span', class_='AppFactTemperature_value__2qhsG').text
    else:
        temperature = "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É."

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ–≥–æ–¥—ã (—á—É–≤—Å—Ç–≤—É–µ—Ç—Å—è –∫–∞–∫)
    feels_like = soup.find('span', class_='AppFact_feels__IJoel')
    if feels_like:
        feels_like_temp = feels_like.text
    else:
        feels_like_temp = "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–µ –æ—â—É—â–∞–µ–º–æ–π."

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–∫–æ—Ä–æ—Å—Ç—å –≤–µ—Ç—Ä–∞
    wind_speed = soup.find('li', class_='AppFact_details__item__QFIXI')
    if wind_speed:
        wind_speed_value = wind_speed.text
    else:
        wind_speed_value = "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ —Å–∫–æ—Ä–æ—Å—Ç–∏ –≤–µ—Ç—Ä–∞."

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–ª–∞–∂–Ω–æ—Å—Ç—å
    humidity = soup.find_all('li', class_='AppFact_details__item__QFIXI')[2]  # –î–ª—è –≤–ª–∞–∂–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ –≤–∑—è—Ç—å —Ç—Ä–µ—Ç–∏–π —ç–ª–µ–º–µ–Ω—Ç
    if humidity:
        humidity_value = humidity.text
    else:
        humidity_value = "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –≤–ª–∞–∂–Ω–æ—Å—Ç–∏."

    return {
        'temperature': temperature,
        'feels_like': feels_like_temp,
        'wind_speed': wind_speed_value,
        'humidity': humidity_value
    }

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

city = "moscow"

weather_info = get_weather(city)

url = "https://api.coingecko.com/api/v3/simple/price"


coins = ['bitcoin', 'solana', 'ethereum', 'binancecoin', 'litecoin', 'ripple']


params = {
    'ids': ','.join(coins),
    'vs_currencies': 'usd'
}


response = requests.get(url, params=params)


if response.status_code == 200:
    data = response.json()
    crypto_res = "\n".join([f"{coin.capitalize()}: ${data[coin]['usd']}" for coin in coins]) #—Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–±–æ—Ä–∞ —Ü–µ–Ω
else:
    crypto_res = "–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –¥–∞–Ω–Ω—ã—Ö."

async def fetch_news(url):
    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    items = soup.select('ul[data-testid="topic-promos"] li')
                    news_list = []

                    for item in items:
                        link_tag = item.select_one('a.bbc-1i4ie53')
                        desc_tag = item.select_one('p')
                        time_tag = item.select_one('time')

                        if link_tag:
                            title = link_tag.text.strip()
                            link = link_tag['href'] if link_tag['href'].startswith("http") else "https://www.bbc.com" + link_tag['href']
                            description = desc_tag.text.strip() if desc_tag else ""
                            timestamp = time_tag.text.strip() if time_tag else ""

                            full_news = await fetch_full_news_text(session, link)
                            news_list.append({
                                "title": title,
                                "link": link,
                                "description": description,
                                "time": timestamp,
                                "full_text": full_news  # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏
                            })

                    return news_list if news_list else "–ù–æ–≤–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
                else:
                    return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {response.status}"
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {e}"

async def fetch_reuters_news():
    url = "https://www.reuters.com"
    headers = {"User-Agent": "Mozilla/5.0"}

    async with aiohttp.ClientSession(headers=headers) as session:
        try:
            async with session.get(url) as response:
                if response.status != 200:
                    return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ Reuters: {response.status}"

                soup = BeautifulSoup(await response.text(), "html.parser")
                articles = soup.select("a[data-testid='Heading']")

                news_list = []
                for article in articles:
                    title = article.get_text(strip=True)
                    href = article.get('href')
                    if not title or not href:
                        continue

                    link = href if href.startswith("http") else f"https://www.reuters.com{href}"
                    full_text = await fetch_full_news_text(session, link)

                    news_list.append({
                        "title": title,
                        "link": link,
                        "description": "",
                        "time": "",
                        "full_text": full_text
                    })

                    if len(news_list) >= 10:
                        break

                return news_list or "–ù–æ–≤–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ Reuters: {e}"

reuters = fetch_reuters_news()


async def fetch_cointelegraph_news():
    url = "https://cointelegraph.com"
    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(url) as response:
                if response.status != 200:
                    return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ Cointelegraph: {response.status}"
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                
                articles = soup.select('a[href^="/news/"]')
                news_list = []

                for article in articles:
                    title = article.get_text(strip=True)
                    href = article.get('href')
                    if not href or not title:
                        continue
                    link = f"https://cointelegraph.com{href}"

                    
                    full_text = await fetch_full_news_text(session, link)

                    news_list.append({
                        "title": title,
                        "link": link,
                        "description": "",  
                        "time": "",         
                        "full_text": full_text
                    })

                    if len(news_list) >= 10:
                        break

                return news_list if news_list else "–ù–æ–≤–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ Cointelegraph: {e}"



async def fetch_full_news_text(session, link):
    try:
        async with session.get(link) as response:
            if response.status != 200:
                return "–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ç–µ–∫—Å—Ç–∞ –Ω–æ–≤–æ—Å—Ç–∏"

            html = await response.text()
            soup = BeautifulSoup(html, 'html.parser')
            domain = urlparse(link).netloc

            if 'bbc.com' in domain:
                return extract_bbc_text(soup)
            elif 'cointelegraph.com' in domain:
                return extract_cointelegraph_text(soup)
            elif 'reuters.com' in domain:
                return extract_reuters_text(soup)
            else:
                return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫"
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Ç–µ–∫—Å—Ç–∞: {e}"

async def generate_summary(news_items):
    api_url = "http://localhost:11434/api/generate"
    headers = {"Content-Type": "application/json"}

    formatted_news = "\n\n".join(
        f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {n['title']}\n–í—Ä–µ–º—è: {n['time']}\n–û–ø–∏—Å–∞–Ω–∏–µ: {n['description']}\n–¢–µ–∫—Å—Ç: {n['full_text']}"
        for n in news_items
    )


    prompt_text = f"""–ü—Ä–∏–≤–µ—Ç, –±–µ–∑ –ª–∏—à–Ω–µ–≥–æ –≤ —Å–≤–æ–µ–º —Å–æ–æ–±—â–µ–Ω–∏–∏ —Å–Ω–∞—á–∞–ª–∞:
1. –ü–æ–ø—Ä–∏–≤–µ—Ç—Å—Ç–≤—É–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –∏–º–µ–Ω–µ–º –Ø–∫–æ–≤ –ê–±—Ä–∞–º–æ–≤.
2. –£–∫–∞–∂–∏ —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É(—Ç–æ–ª—å–∫–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YY  –∏ –≤—Ä–µ–º—è HH:MM:SS):{current_time}
3. –ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–≥–æ–¥–µ: {weather_info}.
4. –ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å —Ç–µ–∫—É—â–∏–µ —Ü–µ–Ω—ã –Ω–∞ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã(–≤ –æ–¥–Ω—É —Å—Ç—Ä–æ—á–∫—É):{crypto_res}
5. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ –Ω–∏–∂–µ –∏ —Å–¥–µ–ª–∞–π —Å–≤—è–∑–Ω–æ–µ, –ª–æ–≥–∏—á–Ω–æ–µ —Ä–µ–∑—é–º–µ, —Å–æ—Å—Ç–æ—è—â–µ–µ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π. –ù–µ –ø–æ–≤—Ç–æ—Ä—è–π –ø—É–Ω–∫—Ç—ã –∑–∞–Ω–æ–≤–æ –¥–ª—è –∫–∞–∂–¥–æ–π –Ω–æ–≤–æ—Å—Ç–∏, –ø—Ä–æ—Å—Ç–æ —Å–¥–µ–ª–∞–π –æ–±–∑–æ—Ä –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤ –≤ –æ–¥–Ω–æ–º –±–ª–æ–∫–µ.

–ù–æ–≤–æ—Å—Ç–∏:
{formatted_news}

–î–æ–ø. —É–∫–∞–∑–∞–Ω–∏—è: {requirements if requirements else "–ù–µ—Ç"}
"""

    payload = {
        "model": "mistral",
        "prompt": prompt_text,
        "stream": False
    }

    async with aiohttp.ClientSession() as session:
        try:
            start_time = time.time()
            async with session.post(api_url, json=payload, headers=headers) as response:
                print("Response time:", time.time() - start_time)

                if response.status == 200:
                    data = await response.json()
                    summary = data.get("response", "–†–µ–∑—é–º–µ –Ω–µ –ø–æ–ª—É—á–µ–Ω–æ.")
                    return summary
                else:
                    return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞: {response.status}"
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ: {e}"

async def periodic_news_check():
    while True:
        print("–ù–∞—á–∞–ª–æ –Ω–æ–≤–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π.")
        bbc_news = await fetch_news(news_url)
        reuters_news = await fetch_reuters_news()
        cointelegraph_news = await fetch_cointelegraph_news()

        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π
        all_news = []
        for news_source in [bbc_news, reuters_news, cointelegraph_news]:
            if isinstance(news_source, list):
                all_news.extend(news_source[:3])  # –ë–µ—Ä–µ–º –ø–æ 3 –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞

        if all_news:
            summary = await generate_summary(all_news)
            if isinstance(summary, str):
                pywhatkit.sendwhatmsg_instantly("+79037530394", summary)
                print(summary)
            else:
                print("–†–µ–∑—é–º–µ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å—Ç—Ä–æ–∫–æ–π, —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ.")
        else:
            print("–û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –Ω–æ–≤–æ—Å—Ç–∏.")

        print("–û–∂–∏–¥–∞–Ω–∏–µ 10 –º–∏–Ω—É—Ç(–∞)...")
        await asyncio.sleep(600)





asyncio.run(periodic_news_check())
